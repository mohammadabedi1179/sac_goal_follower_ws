[
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 3404,
    "train/ent_coef": 0.3597738444805145,
    "train/actor_loss": -2.2918174266815186,
    "train/critic_loss": 0.637553870677948,
    "train/ent_coef_loss": -3.4761712551116943,
    "episode": 10,
    "total_timesteps": 3505,
    "episode_reward": -233.111581,
    "episode_length": 350
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 7027,
    "train/ent_coef": 0.12147580832242966,
    "train/actor_loss": 5.813785076141357,
    "train/critic_loss": 0.44357553124427795,
    "train/ent_coef_loss": -6.971641540527344,
    "episode": 20,
    "total_timesteps": 7128,
    "episode_reward": -285.157927,
    "episode_length": 369
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 10656,
    "train/ent_coef": 0.041205309331417084,
    "train/actor_loss": 15.411232948303223,
    "train/critic_loss": 1.208547830581665,
    "train/ent_coef_loss": -10.216787338256836,
    "episode": 30,
    "total_timesteps": 10757,
    "episode_reward": -281.669545,
    "episode_length": 368
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 14260,
    "train/ent_coef": 0.014539968222379684,
    "train/actor_loss": 25.37442398071289,
    "train/critic_loss": 1.9800419807434082,
    "train/ent_coef_loss": -13.150150299072266,
    "episode": 40,
    "total_timesteps": 14361,
    "episode_reward": -335.972788,
    "episode_length": 340
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 16976,
    "train/ent_coef": 0.00799773819744587,
    "train/actor_loss": 31.85089874267578,
    "train/critic_loss": 1.0435304641723633,
    "train/ent_coef_loss": -9.5806245803833,
    "episode": 50,
    "total_timesteps": 17077,
    "episode_reward": -236.015969,
    "episode_length": 331
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 18410,
    "train/ent_coef": 0.010711130686104298,
    "train/actor_loss": 33.2486572265625,
    "train/critic_loss": 17.778873443603516,
    "train/ent_coef_loss": 6.434785842895508,
    "episode": 60,
    "total_timesteps": 18511,
    "episode_reward": -221.670555,
    "episode_length": 364
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 19746,
    "train/ent_coef": 0.015266766771674156,
    "train/actor_loss": 34.4306526184082,
    "train/critic_loss": 1.9225246906280518,
    "train/ent_coef_loss": 2.6653056144714355,
    "episode": 70,
    "total_timesteps": 19847,
    "episode_reward": -168.849603,
    "episode_length": 363
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 21030,
    "train/ent_coef": 0.02265460602939129,
    "train/actor_loss": 34.508819580078125,
    "train/critic_loss": 5.879604339599609,
    "train/ent_coef_loss": 3.110736608505249,
    "episode": 80,
    "total_timesteps": 21131,
    "episode_reward": -66.809672,
    "episode_length": 135
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 22480,
    "train/ent_coef": 0.03598290681838989,
    "train/actor_loss": 33.83207702636719,
    "train/critic_loss": 6.646727085113525,
    "train/ent_coef_loss": 7.887598037719727,
    "episode": 90,
    "total_timesteps": 22581,
    "episode_reward": -50.975772,
    "episode_length": 297
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 24632,
    "train/ent_coef": 0.058482490479946136,
    "train/actor_loss": 32.783416748046875,
    "train/critic_loss": 3.617691993713379,
    "train/ent_coef_loss": 3.799243211746216,
    "episode": 100,
    "total_timesteps": 24733,
    "episode_reward": 53.942264,
    "episode_length": 26
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 26439,
    "train/ent_coef": 0.0786304771900177,
    "train/actor_loss": 32.57376480102539,
    "train/critic_loss": 11.80664348602295,
    "train/ent_coef_loss": 1.4715293645858765,
    "episode": 110,
    "total_timesteps": 26540,
    "episode_reward": -100.014589,
    "episode_length": 1
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 27474,
    "train/ent_coef": 0.07892108708620071,
    "train/actor_loss": 34.35138702392578,
    "train/critic_loss": 4.3758625984191895,
    "train/ent_coef_loss": -0.3926659822463989,
    "episode": 120,
    "total_timesteps": 27575,
    "episode_reward": -100.044923,
    "episode_length": 3
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 28686,
    "train/ent_coef": 0.07987545430660248,
    "train/actor_loss": 35.171722412109375,
    "train/critic_loss": 80.93623352050781,
    "train/ent_coef_loss": -0.7602816224098206,
    "episode": 130,
    "total_timesteps": 28787,
    "episode_reward": -100.118071,
    "episode_length": 6
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 30799,
    "train/ent_coef": 0.07751598954200745,
    "train/actor_loss": 37.31092071533203,
    "train/critic_loss": 40.99396514892578,
    "train/ent_coef_loss": 0.6717046499252319,
    "episode": 140,
    "total_timesteps": 30900,
    "episode_reward": -77.38396,
    "episode_length": 325
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 31951,
    "train/ent_coef": 0.06342261284589767,
    "train/actor_loss": 37.73279571533203,
    "train/critic_loss": 3.0921735763549805,
    "train/ent_coef_loss": -2.0026166439056396,
    "episode": 150,
    "total_timesteps": 32052,
    "episode_reward": -144.429455,
    "episode_length": 360
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 33624,
    "train/ent_coef": 0.06321573257446289,
    "train/actor_loss": 36.84062576293945,
    "train/critic_loss": 138.10061645507812,
    "train/ent_coef_loss": 0.2782028317451477,
    "episode": 160,
    "total_timesteps": 33725,
    "episode_reward": -95.55683,
    "episode_length": 230
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 36164,
    "train/ent_coef": 0.07651535421609879,
    "train/actor_loss": 38.93394470214844,
    "train/critic_loss": 1.9553816318511963,
    "train/ent_coef_loss": -0.9015604257583618,
    "episode": 170,
    "total_timesteps": 36265,
    "episode_reward": -318.523329,
    "episode_length": 363
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 37072,
    "train/ent_coef": 0.06436306238174438,
    "train/actor_loss": 38.771087646484375,
    "train/critic_loss": 13.678888320922852,
    "train/ent_coef_loss": 0.9700123071670532,
    "episode": 180,
    "total_timesteps": 37173,
    "episode_reward": -101.503123,
    "episode_length": 213
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 38578,
    "train/ent_coef": 0.06645674258470535,
    "train/actor_loss": 39.934234619140625,
    "train/critic_loss": 36.807891845703125,
    "train/ent_coef_loss": -0.3141927421092987,
    "episode": 190,
    "total_timesteps": 38679,
    "episode_reward": -131.239775,
    "episode_length": 145
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 40433,
    "train/ent_coef": 0.06287307292222977,
    "train/actor_loss": 40.884803771972656,
    "train/critic_loss": 1.222778081893921,
    "train/ent_coef_loss": -0.03340432047843933,
    "episode": 200,
    "total_timesteps": 40534,
    "episode_reward": -186.711101,
    "episode_length": 366
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 41813,
    "train/ent_coef": 0.06482292711734772,
    "train/actor_loss": 39.91117858886719,
    "train/critic_loss": 15.500081062316895,
    "train/ent_coef_loss": 0.815532922744751,
    "episode": 210,
    "total_timesteps": 41914,
    "episode_reward": -100.183166,
    "episode_length": 9
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 42309,
    "train/ent_coef": 0.06408443301916122,
    "train/actor_loss": 41.086483001708984,
    "train/critic_loss": 10.76710319519043,
    "train/ent_coef_loss": 0.2612391710281372,
    "episode": 220,
    "total_timesteps": 42410,
    "episode_reward": -100.013155,
    "episode_length": 1
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 43087,
    "train/ent_coef": 0.06290131062269211,
    "train/actor_loss": 41.417625427246094,
    "train/critic_loss": 50.6838264465332,
    "train/ent_coef_loss": -0.11477658152580261,
    "episode": 230,
    "total_timesteps": 43188,
    "episode_reward": -172.880179,
    "episode_length": 113
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 44142,
    "train/ent_coef": 0.06436288356781006,
    "train/actor_loss": 39.62284851074219,
    "train/critic_loss": 57.03670120239258,
    "train/ent_coef_loss": -0.14279130101203918,
    "episode": 240,
    "total_timesteps": 44243,
    "episode_reward": -100.011828,
    "episode_length": 1
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 45449,
    "train/ent_coef": 0.05579056218266487,
    "train/actor_loss": 40.262332916259766,
    "train/critic_loss": 3.086822509765625,
    "train/ent_coef_loss": -1.4052810668945312,
    "episode": 250,
    "total_timesteps": 45550,
    "episode_reward": 14.706478,
    "episode_length": 73
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 46969,
    "train/ent_coef": 0.060578130185604095,
    "train/actor_loss": 41.140499114990234,
    "train/critic_loss": 5.457967758178711,
    "train/ent_coef_loss": -0.268852174282074,
    "episode": 260,
    "total_timesteps": 47070,
    "episode_reward": -100.276412,
    "episode_length": 13
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 48728,
    "train/ent_coef": 0.06074457988142967,
    "train/actor_loss": 41.85614013671875,
    "train/critic_loss": 9.510574340820312,
    "train/ent_coef_loss": 0.3865422308444977,
    "episode": 270,
    "total_timesteps": 48829,
    "episode_reward": -246.860037,
    "episode_length": 220
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 50262,
    "train/ent_coef": 0.05784253403544426,
    "train/actor_loss": 41.7178840637207,
    "train/critic_loss": 9.838788986206055,
    "train/ent_coef_loss": 0.025699861347675323,
    "episode": 280,
    "total_timesteps": 50363,
    "episode_reward": 121.297373,
    "episode_length": 40
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 51961,
    "train/ent_coef": 0.05821475014090538,
    "train/actor_loss": 42.437583923339844,
    "train/critic_loss": 24.054996490478516,
    "train/ent_coef_loss": -0.6738752126693726,
    "episode": 290,
    "total_timesteps": 52062,
    "episode_reward": -231.597411,
    "episode_length": 291
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 53571,
    "train/ent_coef": 0.05726727470755577,
    "train/actor_loss": 42.5941162109375,
    "train/critic_loss": 4.619629383087158,
    "train/ent_coef_loss": -0.7060790657997131,
    "episode": 300,
    "total_timesteps": 53672,
    "episode_reward": -46.350777,
    "episode_length": 88
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 54897,
    "train/ent_coef": 0.06192475184798241,
    "train/actor_loss": 41.81755065917969,
    "train/critic_loss": 2.637218713760376,
    "train/ent_coef_loss": -1.111069679260254,
    "episode": 310,
    "total_timesteps": 54998,
    "episode_reward": -100.158741,
    "episode_length": 12
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 57050,
    "train/ent_coef": 0.05144320800900459,
    "train/actor_loss": 41.421607971191406,
    "train/critic_loss": 13.875988006591797,
    "train/ent_coef_loss": -0.04049083590507507,
    "episode": 320,
    "total_timesteps": 57151,
    "episode_reward": -105.118549,
    "episode_length": 352
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 58190,
    "train/ent_coef": 0.04909295216202736,
    "train/actor_loss": 41.168479919433594,
    "train/critic_loss": 9.8237943649292,
    "train/ent_coef_loss": -1.4020814895629883,
    "episode": 330,
    "total_timesteps": 58291,
    "episode_reward": -182.87215,
    "episode_length": 328
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 59789,
    "train/ent_coef": 0.04933968558907509,
    "train/actor_loss": 41.84884262084961,
    "train/critic_loss": 6.109757423400879,
    "train/ent_coef_loss": -1.3224663734436035,
    "episode": 340,
    "total_timesteps": 59890,
    "episode_reward": -212.330913,
    "episode_length": 181
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 61540,
    "train/ent_coef": 0.051912661641836166,
    "train/actor_loss": 41.93743133544922,
    "train/critic_loss": 14.597549438476562,
    "train/ent_coef_loss": -1.6734780073165894,
    "episode": 350,
    "total_timesteps": 61641,
    "episode_reward": -100.086697,
    "episode_length": 6
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 63307,
    "train/ent_coef": 0.053402457386255264,
    "train/actor_loss": 41.272804260253906,
    "train/critic_loss": 22.30022621154785,
    "train/ent_coef_loss": -1.9682657718658447,
    "episode": 360,
    "total_timesteps": 63408,
    "episode_reward": -37.128981,
    "episode_length": 371
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 64817,
    "train/ent_coef": 0.052661582827568054,
    "train/actor_loss": 41.474090576171875,
    "train/critic_loss": 1.3436105251312256,
    "train/ent_coef_loss": 0.09217986464500427,
    "episode": 370,
    "total_timesteps": 64918,
    "episode_reward": 65.782231,
    "episode_length": 365
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 66395,
    "train/ent_coef": 0.04833175614476204,
    "train/actor_loss": 42.02056884765625,
    "train/critic_loss": 17.290754318237305,
    "train/ent_coef_loss": -1.0403552055358887,
    "episode": 380,
    "total_timesteps": 66496,
    "episode_reward": -361.175809,
    "episode_length": 275
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 67776,
    "train/ent_coef": 0.049741413444280624,
    "train/actor_loss": 40.949615478515625,
    "train/critic_loss": 19.48807144165039,
    "train/ent_coef_loss": -0.39171236753463745,
    "episode": 390,
    "total_timesteps": 67877,
    "episode_reward": 31.842463,
    "episode_length": 375
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 69017,
    "train/ent_coef": 0.046732768416404724,
    "train/actor_loss": 40.94478988647461,
    "train/critic_loss": 95.35856628417969,
    "train/ent_coef_loss": 1.3363392353057861,
    "episode": 400,
    "total_timesteps": 69118,
    "episode_reward": -100.030712,
    "episode_length": 2
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 70763,
    "train/ent_coef": 0.04474903643131256,
    "train/actor_loss": 41.891258239746094,
    "train/critic_loss": 12.69161319732666,
    "train/ent_coef_loss": 0.38422536849975586,
    "episode": 410,
    "total_timesteps": 70864,
    "episode_reward": -100.05282,
    "episode_length": 3
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 71126,
    "train/ent_coef": 0.044754788279533386,
    "train/actor_loss": 43.16459274291992,
    "train/critic_loss": 19.8302001953125,
    "train/ent_coef_loss": -1.0441800355911255,
    "episode": 420,
    "total_timesteps": 71227,
    "episode_reward": -100.012341,
    "episode_length": 1
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 73042,
    "train/ent_coef": 0.041931845247745514,
    "train/actor_loss": 42.16444396972656,
    "train/critic_loss": 158.80154418945312,
    "train/ent_coef_loss": 0.5853055119514465,
    "episode": 430,
    "total_timesteps": 73143,
    "episode_reward": -36.059549,
    "episode_length": 366
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 74331,
    "train/ent_coef": 0.04600070044398308,
    "train/actor_loss": 42.33256530761719,
    "train/critic_loss": 23.44992446899414,
    "train/ent_coef_loss": 0.9213979244232178,
    "episode": 440,
    "total_timesteps": 74432,
    "episode_reward": -100.012205,
    "episode_length": 1
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 75551,
    "train/ent_coef": 0.04752817004919052,
    "train/actor_loss": 41.734039306640625,
    "train/critic_loss": 4.247358322143555,
    "train/ent_coef_loss": 0.7268389463424683,
    "episode": 450,
    "total_timesteps": 75652,
    "episode_reward": -100.355024,
    "episode_length": 15
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 76966,
    "train/ent_coef": 0.04310867562890053,
    "train/actor_loss": 41.52968215942383,
    "train/critic_loss": 7.297976493835449,
    "train/ent_coef_loss": -0.9464588165283203,
    "episode": 460,
    "total_timesteps": 77067,
    "episode_reward": -100.023029,
    "episode_length": 2
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 78113,
    "train/ent_coef": 0.041508596390485764,
    "train/actor_loss": 41.40167236328125,
    "train/critic_loss": 7.918998718261719,
    "train/ent_coef_loss": 0.3854843080043793,
    "episode": 470,
    "total_timesteps": 78214,
    "episode_reward": -100.014814,
    "episode_length": 1
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 79356,
    "train/ent_coef": 0.04053439944982529,
    "train/actor_loss": 42.007362365722656,
    "train/critic_loss": 14.147289276123047,
    "train/ent_coef_loss": -0.3765881657600403,
    "episode": 480,
    "total_timesteps": 79457,
    "episode_reward": -167.696031,
    "episode_length": 112
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 80790,
    "train/ent_coef": 0.044705964624881744,
    "train/actor_loss": 42.34748077392578,
    "train/critic_loss": 13.265328407287598,
    "train/ent_coef_loss": 0.6189278364181519,
    "episode": 490,
    "total_timesteps": 80891,
    "episode_reward": -24.330197,
    "episode_length": 362
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 82059,
    "train/ent_coef": 0.043025996536016464,
    "train/actor_loss": 41.96827697753906,
    "train/critic_loss": 4.120311737060547,
    "train/ent_coef_loss": -1.0173563957214355,
    "episode": 500,
    "total_timesteps": 82160,
    "episode_reward": -100.153168,
    "episode_length": 8
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 83491,
    "train/ent_coef": 0.042287345975637436,
    "train/actor_loss": 40.88368606567383,
    "train/critic_loss": 5.997445106506348,
    "train/ent_coef_loss": -1.4467594623565674,
    "episode": 510,
    "total_timesteps": 83592,
    "episode_reward": -36.92599,
    "episode_length": 273
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 84982,
    "train/ent_coef": 0.04238322749733925,
    "train/actor_loss": 42.2242546081543,
    "train/critic_loss": 4.554690361022949,
    "train/ent_coef_loss": -1.9089648723602295,
    "episode": 520,
    "total_timesteps": 85083,
    "episode_reward": -100.161868,
    "episode_length": 12
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 86744,
    "train/ent_coef": 0.04232679679989815,
    "train/actor_loss": 42.85040283203125,
    "train/critic_loss": 207.16253662109375,
    "train/ent_coef_loss": 0.20502611994743347,
    "episode": 530,
    "total_timesteps": 86845,
    "episode_reward": -100.01213,
    "episode_length": 1
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 87699,
    "train/ent_coef": 0.04065551608800888,
    "train/actor_loss": 43.2342529296875,
    "train/critic_loss": 3.2353081703186035,
    "train/ent_coef_loss": -2.6309919357299805,
    "episode": 540,
    "total_timesteps": 87800,
    "episode_reward": -100.096612,
    "episode_length": 7
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 89845,
    "train/ent_coef": 0.04528120905160904,
    "train/actor_loss": 42.00148010253906,
    "train/critic_loss": 9.070213317871094,
    "train/ent_coef_loss": 1.876695990562439,
    "episode": 550,
    "total_timesteps": 89946,
    "episode_reward": 30.356677,
    "episode_length": 68
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 90044,
    "train/ent_coef": 0.04610089585185051,
    "train/actor_loss": 43.144351959228516,
    "train/critic_loss": 19.27196502685547,
    "train/ent_coef_loss": 0.9661498665809631,
    "episode": 560,
    "total_timesteps": 90145,
    "episode_reward": -100.111321,
    "episode_length": 7
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 91782,
    "train/ent_coef": 0.04389013350009918,
    "train/actor_loss": 42.71233367919922,
    "train/critic_loss": 54.56800842285156,
    "train/ent_coef_loss": -0.3781389594078064,
    "episode": 570,
    "total_timesteps": 91883,
    "episode_reward": -33.450702,
    "episode_length": 368
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 92661,
    "train/ent_coef": 0.04422444477677345,
    "train/actor_loss": 43.873470306396484,
    "train/critic_loss": 70.407958984375,
    "train/ent_coef_loss": -0.3844064772129059,
    "episode": 580,
    "total_timesteps": 92762,
    "episode_reward": -31.409855,
    "episode_length": 374
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 94109,
    "train/ent_coef": 0.04755626991391182,
    "train/actor_loss": 41.88572692871094,
    "train/critic_loss": 3.7759485244750977,
    "train/ent_coef_loss": -1.4911253452301025,
    "episode": 590,
    "total_timesteps": 94210,
    "episode_reward": 34.282818,
    "episode_length": 374
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 95532,
    "train/ent_coef": 0.04666106030344963,
    "train/actor_loss": 42.79197692871094,
    "train/critic_loss": 38.558685302734375,
    "train/ent_coef_loss": 0.4946632385253906,
    "episode": 600,
    "total_timesteps": 95633,
    "episode_reward": -100.133798,
    "episode_length": 10
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 97494,
    "train/ent_coef": 0.04586073011159897,
    "train/actor_loss": 43.86253356933594,
    "train/critic_loss": 3.524392604827881,
    "train/ent_coef_loss": 1.249618649482727,
    "episode": 610,
    "total_timesteps": 97595,
    "episode_reward": 1.172074,
    "episode_length": 372
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 98490,
    "train/ent_coef": 0.04690412059426308,
    "train/actor_loss": 42.46938705444336,
    "train/critic_loss": 52.11035919189453,
    "train/ent_coef_loss": 1.701181411743164,
    "episode": 620,
    "total_timesteps": 98591,
    "episode_reward": -131.535191,
    "episode_length": 255
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 99383,
    "train/ent_coef": 0.05093095451593399,
    "train/actor_loss": 42.68247604370117,
    "train/critic_loss": 1.865763783454895,
    "train/ent_coef_loss": 1.1646912097930908,
    "episode": 630,
    "total_timesteps": 99484,
    "episode_reward": -14.510565,
    "episode_length": 353
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 100713,
    "train/ent_coef": 0.04522976279258728,
    "train/actor_loss": 42.74763488769531,
    "train/critic_loss": 60.67084503173828,
    "train/ent_coef_loss": 0.2536672353744507,
    "episode": 640,
    "total_timesteps": 100814,
    "episode_reward": -479.771386,
    "episode_length": 348
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 102398,
    "train/ent_coef": 0.03814847767353058,
    "train/actor_loss": 42.2728385925293,
    "train/critic_loss": 33.812156677246094,
    "train/ent_coef_loss": 0.06975753605365753,
    "episode": 650,
    "total_timesteps": 102499,
    "episode_reward": -161.220936,
    "episode_length": 372
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 104131,
    "train/ent_coef": 0.0378803014755249,
    "train/actor_loss": 43.62613296508789,
    "train/critic_loss": 5.72406005859375,
    "train/ent_coef_loss": -0.32273659110069275,
    "episode": 660,
    "total_timesteps": 104232,
    "episode_reward": -62.169669,
    "episode_length": 346
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 105104,
    "train/ent_coef": 0.039250727742910385,
    "train/actor_loss": 42.86350631713867,
    "train/critic_loss": 6.998409748077393,
    "train/ent_coef_loss": -2.043403148651123,
    "episode": 670,
    "total_timesteps": 105205,
    "episode_reward": -31.122115,
    "episode_length": 198
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 106593,
    "train/ent_coef": 0.038573626428842545,
    "train/actor_loss": 43.3628044128418,
    "train/critic_loss": 3.303945302963257,
    "train/ent_coef_loss": 0.48055726289749146,
    "episode": 680,
    "total_timesteps": 106694,
    "episode_reward": 27.432903,
    "episode_length": 45
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 108231,
    "train/ent_coef": 0.03777327015995979,
    "train/actor_loss": 44.28923034667969,
    "train/critic_loss": 10.067124366760254,
    "train/ent_coef_loss": -1.894080638885498,
    "episode": 690,
    "total_timesteps": 108332,
    "episode_reward": -78.047519,
    "episode_length": 368
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 110147,
    "train/ent_coef": 0.036537520587444305,
    "train/actor_loss": 42.53616714477539,
    "train/critic_loss": 4.29753303527832,
    "train/ent_coef_loss": -0.9831742644309998,
    "episode": 700,
    "total_timesteps": 110248,
    "episode_reward": -91.93442,
    "episode_length": 172
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 112288,
    "train/ent_coef": 0.03619960695505142,
    "train/actor_loss": 43.44948959350586,
    "train/critic_loss": 24.63007926940918,
    "train/ent_coef_loss": -0.3976426422595978,
    "episode": 710,
    "total_timesteps": 112389,
    "episode_reward": -100.330254,
    "episode_length": 17
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 113412,
    "train/ent_coef": 0.03730200603604317,
    "train/actor_loss": 41.59073257446289,
    "train/critic_loss": 6.384284496307373,
    "train/ent_coef_loss": 0.22922873497009277,
    "episode": 720,
    "total_timesteps": 113513,
    "episode_reward": -100.199368,
    "episode_length": 10
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 115997,
    "train/ent_coef": 0.0331093966960907,
    "train/actor_loss": 41.67630386352539,
    "train/critic_loss": 12.094295501708984,
    "train/ent_coef_loss": -0.6675053238868713,
    "episode": 730,
    "total_timesteps": 116098,
    "episode_reward": -11.834567,
    "episode_length": 364
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 117344,
    "train/ent_coef": 0.032044995576143265,
    "train/actor_loss": 41.583778381347656,
    "train/critic_loss": 5.19437837600708,
    "train/ent_coef_loss": 0.4013480842113495,
    "episode": 740,
    "total_timesteps": 117445,
    "episode_reward": -100.012733,
    "episode_length": 1
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 119581,
    "train/ent_coef": 0.03230184316635132,
    "train/actor_loss": 41.894012451171875,
    "train/critic_loss": 9.799175262451172,
    "train/ent_coef_loss": -0.27186650037765503,
    "episode": 750,
    "total_timesteps": 119682,
    "episode_reward": -100.014326,
    "episode_length": 1
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 121520,
    "train/ent_coef": 0.029201777651906013,
    "train/actor_loss": 43.45674514770508,
    "train/critic_loss": 5.342172622680664,
    "train/ent_coef_loss": 2.410526752471924,
    "episode": 760,
    "total_timesteps": 121621,
    "episode_reward": -100.013349,
    "episode_length": 1
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 123194,
    "train/ent_coef": 0.030632561072707176,
    "train/actor_loss": 44.726951599121094,
    "train/critic_loss": 16.874616622924805,
    "train/ent_coef_loss": -0.17596220970153809,
    "episode": 770,
    "total_timesteps": 123295,
    "episode_reward": -100.181238,
    "episode_length": 11
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 124591,
    "train/ent_coef": 0.02907637320458889,
    "train/actor_loss": 42.51829528808594,
    "train/critic_loss": 117.06153106689453,
    "train/ent_coef_loss": 1.30153489112854,
    "episode": 780,
    "total_timesteps": 124692,
    "episode_reward": -100.014074,
    "episode_length": 1
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 126307,
    "train/ent_coef": 0.03137451782822609,
    "train/actor_loss": 41.431827545166016,
    "train/critic_loss": 3.932640790939331,
    "train/ent_coef_loss": 0.8596082925796509,
    "episode": 790,
    "total_timesteps": 126408,
    "episode_reward": -100.112204,
    "episode_length": 6
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 127537,
    "train/ent_coef": 0.03299114108085632,
    "train/actor_loss": 41.97011947631836,
    "train/critic_loss": 47.12834930419922,
    "train/ent_coef_loss": 0.2695346474647522,
    "episode": 800,
    "total_timesteps": 127638,
    "episode_reward": -100.1656,
    "episode_length": 10
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 129530,
    "train/ent_coef": 0.030313173308968544,
    "train/actor_loss": 41.14904022216797,
    "train/critic_loss": 9.299051284790039,
    "train/ent_coef_loss": 0.34522125124931335,
    "episode": 810,
    "total_timesteps": 129631,
    "episode_reward": -93.413631,
    "episode_length": 361
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 131074,
    "train/ent_coef": 0.02851417474448681,
    "train/actor_loss": 41.39169692993164,
    "train/critic_loss": 4.031399726867676,
    "train/ent_coef_loss": -0.37903934717178345,
    "episode": 820,
    "total_timesteps": 131175,
    "episode_reward": -58.989211,
    "episode_length": 364
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 132778,
    "train/ent_coef": 0.031315311789512634,
    "train/actor_loss": 41.36100769042969,
    "train/critic_loss": 10.921106338500977,
    "train/ent_coef_loss": 0.48683351278305054,
    "episode": 830,
    "total_timesteps": 132879,
    "episode_reward": -100.013522,
    "episode_length": 1
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 134494,
    "train/ent_coef": 0.03334368020296097,
    "train/actor_loss": 41.343509674072266,
    "train/critic_loss": 23.269502639770508,
    "train/ent_coef_loss": -0.8718836903572083,
    "episode": 840,
    "total_timesteps": 134595,
    "episode_reward": -61.33297,
    "episode_length": 362
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 135985,
    "train/ent_coef": 0.03179188072681427,
    "train/actor_loss": 42.16169357299805,
    "train/critic_loss": 12.560754776000977,
    "train/ent_coef_loss": 0.3939821124076843,
    "episode": 850,
    "total_timesteps": 136086,
    "episode_reward": -100.029518,
    "episode_length": 2
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 137615,
    "train/ent_coef": 0.02942662313580513,
    "train/actor_loss": 40.84246826171875,
    "train/critic_loss": 6.150461673736572,
    "train/ent_coef_loss": 0.07291531562805176,
    "episode": 860,
    "total_timesteps": 137716,
    "episode_reward": -134.473916,
    "episode_length": 366
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 138689,
    "train/ent_coef": 0.03239349648356438,
    "train/actor_loss": 40.962554931640625,
    "train/critic_loss": 21.12636947631836,
    "train/ent_coef_loss": 0.2286175787448883,
    "episode": 870,
    "total_timesteps": 138790,
    "episode_reward": -100.013173,
    "episode_length": 1
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 140146,
    "train/ent_coef": 0.03344486653804779,
    "train/actor_loss": 40.87183380126953,
    "train/critic_loss": 55.87214660644531,
    "train/ent_coef_loss": -0.48159539699554443,
    "episode": 880,
    "total_timesteps": 140247,
    "episode_reward": -97.674651,
    "episode_length": 369
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 141705,
    "train/ent_coef": 0.035505276173353195,
    "train/actor_loss": 41.31159210205078,
    "train/critic_loss": 6.257933139801025,
    "train/ent_coef_loss": 1.3983776569366455,
    "episode": 890,
    "total_timesteps": 141806,
    "episode_reward": -75.187215,
    "episode_length": 377
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 143540,
    "train/ent_coef": 0.03444347530603409,
    "train/actor_loss": 40.34498596191406,
    "train/critic_loss": 9.32603931427002,
    "train/ent_coef_loss": -0.4855932295322418,
    "episode": 900,
    "total_timesteps": 143641,
    "episode_reward": -100.195494,
    "episode_length": 12
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 145414,
    "train/ent_coef": 0.035047296434640884,
    "train/actor_loss": 40.696746826171875,
    "train/critic_loss": 16.741321563720703,
    "train/ent_coef_loss": -0.39468634128570557,
    "episode": 910,
    "total_timesteps": 145515,
    "episode_reward": -100.089099,
    "episode_length": 7
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 147161,
    "train/ent_coef": 0.03392568975687027,
    "train/actor_loss": 40.09899139404297,
    "train/critic_loss": 1.638221263885498,
    "train/ent_coef_loss": 0.5555956959724426,
    "episode": 920,
    "total_timesteps": 147262,
    "episode_reward": -100.01408,
    "episode_length": 1
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 148010,
    "train/ent_coef": 0.03548344224691391,
    "train/actor_loss": 40.63645935058594,
    "train/critic_loss": 19.62899398803711,
    "train/ent_coef_loss": -1.1452739238739014,
    "episode": 930,
    "total_timesteps": 148111,
    "episode_reward": -100.438687,
    "episode_length": 17
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 148873,
    "train/ent_coef": 0.0363239049911499,
    "train/actor_loss": 40.57758331298828,
    "train/critic_loss": 11.120282173156738,
    "train/ent_coef_loss": 0.8379989266395569,
    "episode": 940,
    "total_timesteps": 148974,
    "episode_reward": -100.025126,
    "episode_length": 2
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 150635,
    "train/ent_coef": 0.03741396218538284,
    "train/actor_loss": 40.93256378173828,
    "train/critic_loss": 17.797943115234375,
    "train/ent_coef_loss": 0.49706974625587463,
    "episode": 950,
    "total_timesteps": 150736,
    "episode_reward": 38.521029,
    "episode_length": 80
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 153036,
    "train/ent_coef": 0.03837558627128601,
    "train/actor_loss": 40.261749267578125,
    "train/critic_loss": 14.45914077758789,
    "train/ent_coef_loss": 0.6129093170166016,
    "episode": 960,
    "total_timesteps": 153137,
    "episode_reward": -26.392213,
    "episode_length": 370
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 153857,
    "train/ent_coef": 0.03659840300679207,
    "train/actor_loss": 41.315826416015625,
    "train/critic_loss": 105.86672973632812,
    "train/ent_coef_loss": -1.1827311515808105,
    "episode": 970,
    "total_timesteps": 153958,
    "episode_reward": -100.363127,
    "episode_length": 16
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 155816,
    "train/ent_coef": 0.03409906104207039,
    "train/actor_loss": 41.139869689941406,
    "train/critic_loss": 3.867337942123413,
    "train/ent_coef_loss": -0.48754313588142395,
    "episode": 980,
    "total_timesteps": 155917,
    "episode_reward": -100.253306,
    "episode_length": 15
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 157648,
    "train/ent_coef": 0.03366528078913689,
    "train/actor_loss": 40.79331588745117,
    "train/critic_loss": 16.708209991455078,
    "train/ent_coef_loss": 1.0269505977630615,
    "episode": 990,
    "total_timesteps": 157749,
    "episode_reward": 13.62232,
    "episode_length": 373
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 159071,
    "train/ent_coef": 0.034162987023591995,
    "train/actor_loss": 40.84290313720703,
    "train/critic_loss": 2.7808313369750977,
    "train/ent_coef_loss": 0.41571611166000366,
    "episode": 1000,
    "total_timesteps": 159172,
    "episode_reward": -100.227638,
    "episode_length": 16
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 161271,
    "train/ent_coef": 0.032472528517246246,
    "train/actor_loss": 40.61363220214844,
    "train/critic_loss": 5.040627956390381,
    "train/ent_coef_loss": 0.6162927746772766,
    "episode": 1010,
    "total_timesteps": 161372,
    "episode_reward": -100.014815,
    "episode_length": 1
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 162055,
    "train/ent_coef": 0.03276747092604637,
    "train/actor_loss": 40.96205139160156,
    "train/critic_loss": 15.268881797790527,
    "train/ent_coef_loss": -0.05291977524757385,
    "episode": 1020,
    "total_timesteps": 162156,
    "episode_reward": -100.162052,
    "episode_length": 10
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 162493,
    "train/ent_coef": 0.03453958407044411,
    "train/actor_loss": 41.063941955566406,
    "train/critic_loss": 4.193446159362793,
    "train/ent_coef_loss": -0.09992271661758423,
    "episode": 1030,
    "total_timesteps": 162594,
    "episode_reward": 48.787547,
    "episode_length": 32
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 163992,
    "train/ent_coef": 0.0348648764193058,
    "train/actor_loss": 40.14753723144531,
    "train/critic_loss": 12.344985961914062,
    "train/ent_coef_loss": 0.561827540397644,
    "episode": 1040,
    "total_timesteps": 164093,
    "episode_reward": -100.138241,
    "episode_length": 9
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 165683,
    "train/ent_coef": 0.03526511415839195,
    "train/actor_loss": 40.38017272949219,
    "train/critic_loss": 7.472414970397949,
    "train/ent_coef_loss": -0.20759713649749756,
    "episode": 1050,
    "total_timesteps": 165784,
    "episode_reward": -175.128581,
    "episode_length": 374
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 167420,
    "train/ent_coef": 0.03435250371694565,
    "train/actor_loss": 40.54402160644531,
    "train/critic_loss": 63.69955825805664,
    "train/ent_coef_loss": -0.7558188438415527,
    "episode": 1060,
    "total_timesteps": 167521,
    "episode_reward": 96.995108,
    "episode_length": 18
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 169516,
    "train/ent_coef": 0.03602077439427376,
    "train/actor_loss": 40.16809844970703,
    "train/critic_loss": 76.97364807128906,
    "train/ent_coef_loss": 1.7303918600082397,
    "episode": 1070,
    "total_timesteps": 169617,
    "episode_reward": -169.56548,
    "episode_length": 102
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 171364,
    "train/ent_coef": 0.03216639161109924,
    "train/actor_loss": 39.16997528076172,
    "train/critic_loss": 75.88670349121094,
    "train/ent_coef_loss": 1.1080628633499146,
    "episode": 1080,
    "total_timesteps": 171465,
    "episode_reward": -165.97624,
    "episode_length": 83
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 173870,
    "train/ent_coef": 0.03393528237938881,
    "train/actor_loss": 40.21057891845703,
    "train/critic_loss": 15.582748413085938,
    "train/ent_coef_loss": -0.6329240798950195,
    "episode": 1090,
    "total_timesteps": 173971,
    "episode_reward": -106.41131,
    "episode_length": 372
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 174252,
    "train/ent_coef": 0.034293223172426224,
    "train/actor_loss": 39.13861083984375,
    "train/critic_loss": 119.5445556640625,
    "train/ent_coef_loss": 0.3096604645252228,
    "episode": 1100,
    "total_timesteps": 174353,
    "episode_reward": -100.014505,
    "episode_length": 1
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 176120,
    "train/ent_coef": 0.03500434011220932,
    "train/actor_loss": 39.78099822998047,
    "train/critic_loss": 9.37092399597168,
    "train/ent_coef_loss": 0.6486847400665283,
    "episode": 1110,
    "total_timesteps": 176221,
    "episode_reward": 99.264988,
    "episode_length": 293
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 176755,
    "train/ent_coef": 0.03587692230939865,
    "train/actor_loss": 38.689842224121094,
    "train/critic_loss": 11.513628005981445,
    "train/ent_coef_loss": 0.42685243487358093,
    "episode": 1120,
    "total_timesteps": 176856,
    "episode_reward": -100.208443,
    "episode_length": 10
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 179105,
    "train/ent_coef": 0.039884302765131,
    "train/actor_loss": 39.23672866821289,
    "train/critic_loss": 10.91151237487793,
    "train/ent_coef_loss": -1.5436023473739624,
    "episode": 1130,
    "total_timesteps": 179206,
    "episode_reward": -38.138543,
    "episode_length": 373
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 180333,
    "train/ent_coef": 0.038119010627269745,
    "train/actor_loss": 37.27794647216797,
    "train/critic_loss": 39.01188659667969,
    "train/ent_coef_loss": -1.1267729997634888,
    "episode": 1140,
    "total_timesteps": 180434,
    "episode_reward": -117.138804,
    "episode_length": 138
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 182205,
    "train/ent_coef": 0.038521863520145416,
    "train/actor_loss": 37.70085906982422,
    "train/critic_loss": 59.371009826660156,
    "train/ent_coef_loss": -0.18476614356040955,
    "episode": 1150,
    "total_timesteps": 182306,
    "episode_reward": -100.014693,
    "episode_length": 1
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 183614,
    "train/ent_coef": 0.03679288178682327,
    "train/actor_loss": 37.49195098876953,
    "train/critic_loss": 71.93293762207031,
    "train/ent_coef_loss": 0.5026464462280273,
    "episode": 1160,
    "total_timesteps": 183715,
    "episode_reward": 0.159055,
    "episode_length": 358
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 183844,
    "train/ent_coef": 0.036206528544425964,
    "train/actor_loss": 38.39300537109375,
    "train/critic_loss": 37.047698974609375,
    "train/ent_coef_loss": 0.5314592719078064,
    "episode": 1170,
    "total_timesteps": 183945,
    "episode_reward": -100.169835,
    "episode_length": 8
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 185456,
    "train/ent_coef": 0.03512866050004959,
    "train/actor_loss": 37.69515609741211,
    "train/critic_loss": 5.227099418640137,
    "train/ent_coef_loss": 2.020524740219116,
    "episode": 1180,
    "total_timesteps": 185557,
    "episode_reward": -33.512814,
    "episode_length": 375
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 186590,
    "train/ent_coef": 0.03451383486390114,
    "train/actor_loss": 37.648773193359375,
    "train/critic_loss": 5.886667251586914,
    "train/ent_coef_loss": 0.6710243225097656,
    "episode": 1190,
    "total_timesteps": 186691,
    "episode_reward": -100.014211,
    "episode_length": 1
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 187916,
    "train/ent_coef": 0.035282280296087265,
    "train/actor_loss": 36.68810272216797,
    "train/critic_loss": 9.742465019226074,
    "train/ent_coef_loss": 2.5735392570495605,
    "episode": 1200,
    "total_timesteps": 188017,
    "episode_reward": -102.632992,
    "episode_length": 367
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 189885,
    "train/ent_coef": 0.039516475051641464,
    "train/actor_loss": 37.722564697265625,
    "train/critic_loss": 6.569079399108887,
    "train/ent_coef_loss": 1.0659172534942627,
    "episode": 1210,
    "total_timesteps": 189986,
    "episode_reward": -100.014212,
    "episode_length": 1
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 191910,
    "train/ent_coef": 0.0388944074511528,
    "train/actor_loss": 36.53554153442383,
    "train/critic_loss": 8.717897415161133,
    "train/ent_coef_loss": -1.1131612062454224,
    "episode": 1220,
    "total_timesteps": 192011,
    "episode_reward": -100.012103,
    "episode_length": 1
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 192936,
    "train/ent_coef": 0.03797883912920952,
    "train/actor_loss": 36.71233367919922,
    "train/critic_loss": 16.189353942871094,
    "train/ent_coef_loss": -0.11590442061424255,
    "episode": 1230,
    "total_timesteps": 193037,
    "episode_reward": 27.496534,
    "episode_length": 42
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 193346,
    "train/ent_coef": 0.03808870539069176,
    "train/actor_loss": 37.4344596862793,
    "train/critic_loss": 67.41465759277344,
    "train/ent_coef_loss": 1.6385364532470703,
    "episode": 1240,
    "total_timesteps": 193447,
    "episode_reward": -100.014057,
    "episode_length": 1
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 194413,
    "train/ent_coef": 0.039120275527238846,
    "train/actor_loss": 36.30183792114258,
    "train/critic_loss": 74.73295593261719,
    "train/ent_coef_loss": 0.6853545308113098,
    "episode": 1250,
    "total_timesteps": 194514,
    "episode_reward": -141.47489,
    "episode_length": 321
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 195423,
    "train/ent_coef": 0.03791971132159233,
    "train/actor_loss": 36.51661682128906,
    "train/critic_loss": 5.84183406829834,
    "train/ent_coef_loss": 0.9739899039268494,
    "episode": 1260,
    "total_timesteps": 195524,
    "episode_reward": -193.714431,
    "episode_length": 343
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 196678,
    "train/ent_coef": 0.03629853203892708,
    "train/actor_loss": 37.176910400390625,
    "train/critic_loss": 76.33586120605469,
    "train/ent_coef_loss": -0.345856636762619,
    "episode": 1270,
    "total_timesteps": 196779,
    "episode_reward": -100.014039,
    "episode_length": 1
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 197145,
    "train/ent_coef": 0.037077173590660095,
    "train/actor_loss": 36.626708984375,
    "train/critic_loss": 3.091676712036133,
    "train/ent_coef_loss": -1.060415506362915,
    "episode": 1280,
    "total_timesteps": 197246,
    "episode_reward": -100.060568,
    "episode_length": 4
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 198647,
    "train/ent_coef": 0.03594621643424034,
    "train/actor_loss": 37.96752166748047,
    "train/critic_loss": 7.6848602294921875,
    "train/ent_coef_loss": 0.22105823457241058,
    "episode": 1290,
    "total_timesteps": 198748,
    "episode_reward": -100.117744,
    "episode_length": 7
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 200041,
    "train/ent_coef": 0.03531426563858986,
    "train/actor_loss": 36.600284576416016,
    "train/critic_loss": 11.898372650146484,
    "train/ent_coef_loss": 0.9227688312530518,
    "episode": 1300,
    "total_timesteps": 200142,
    "episode_reward": -100.01175,
    "episode_length": 1
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 201522,
    "train/ent_coef": 0.03463444113731384,
    "train/actor_loss": 36.26952362060547,
    "train/critic_loss": 9.639704704284668,
    "train/ent_coef_loss": 0.7539975643157959,
    "episode": 1310,
    "total_timesteps": 201623,
    "episode_reward": -100.241121,
    "episode_length": 16
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 202660,
    "train/ent_coef": 0.03813529014587402,
    "train/actor_loss": 37.34033966064453,
    "train/critic_loss": 7.2271294593811035,
    "train/ent_coef_loss": 0.26742300391197205,
    "episode": 1320,
    "total_timesteps": 202761,
    "episode_reward": -100.014074,
    "episode_length": 1
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 204247,
    "train/ent_coef": 0.03620212897658348,
    "train/actor_loss": 36.74993896484375,
    "train/critic_loss": 74.48963928222656,
    "train/ent_coef_loss": 0.2793482840061188,
    "episode": 1330,
    "total_timesteps": 204348,
    "episode_reward": -208.720084,
    "episode_length": 114
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 206090,
    "train/ent_coef": 0.03164675086736679,
    "train/actor_loss": 37.139129638671875,
    "train/critic_loss": 11.970268249511719,
    "train/ent_coef_loss": 1.8147239685058594,
    "episode": 1340,
    "total_timesteps": 206191,
    "episode_reward": -38.347578,
    "episode_length": 347
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 207324,
    "train/ent_coef": 0.030814241617918015,
    "train/actor_loss": 36.83369445800781,
    "train/critic_loss": 5.097440719604492,
    "train/ent_coef_loss": 1.3437031507492065,
    "episode": 1350,
    "total_timesteps": 207425,
    "episode_reward": 6.14692,
    "episode_length": 116
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 208093,
    "train/ent_coef": 0.03217483311891556,
    "train/actor_loss": 36.75638961791992,
    "train/critic_loss": 3.730571746826172,
    "train/ent_coef_loss": -0.5995121002197266,
    "episode": 1360,
    "total_timesteps": 208194,
    "episode_reward": -152.158537,
    "episode_length": 120
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 209612,
    "train/ent_coef": 0.034774299710989,
    "train/actor_loss": 36.59127426147461,
    "train/critic_loss": 73.36238861083984,
    "train/ent_coef_loss": 1.737040400505066,
    "episode": 1370,
    "total_timesteps": 209713,
    "episode_reward": 75.355128,
    "episode_length": 133
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 212114,
    "train/ent_coef": 0.03441751375794411,
    "train/actor_loss": 37.24958038330078,
    "train/critic_loss": 10.243322372436523,
    "train/ent_coef_loss": 0.9417662024497986,
    "episode": 1380,
    "total_timesteps": 212215,
    "episode_reward": 8.881482,
    "episode_length": 372
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 212610,
    "train/ent_coef": 0.03565937653183937,
    "train/actor_loss": 37.93134307861328,
    "train/critic_loss": 7.770466327667236,
    "train/ent_coef_loss": 0.614966094493866,
    "episode": 1390,
    "total_timesteps": 212711,
    "episode_reward": -100.049041,
    "episode_length": 3
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 213903,
    "train/ent_coef": 0.03487376496195793,
    "train/actor_loss": 37.131568908691406,
    "train/critic_loss": 18.582225799560547,
    "train/ent_coef_loss": -0.34762850403785706,
    "episode": 1400,
    "total_timesteps": 214004,
    "episode_reward": 23.761262,
    "episode_length": 354
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 215866,
    "train/ent_coef": 0.03316754475235939,
    "train/actor_loss": 36.683807373046875,
    "train/critic_loss": 19.41679573059082,
    "train/ent_coef_loss": -1.020412802696228,
    "episode": 1410,
    "total_timesteps": 215967,
    "episode_reward": 28.697213,
    "episode_length": 360
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 217355,
    "train/ent_coef": 0.033939845860004425,
    "train/actor_loss": 36.93690490722656,
    "train/critic_loss": 104.11322784423828,
    "train/ent_coef_loss": 0.17732304334640503,
    "episode": 1420,
    "total_timesteps": 217456,
    "episode_reward": -100.087614,
    "episode_length": 6
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 218811,
    "train/ent_coef": 0.033748842775821686,
    "train/actor_loss": 36.831260681152344,
    "train/critic_loss": 1.829766035079956,
    "train/ent_coef_loss": -0.7413350343704224,
    "episode": 1430,
    "total_timesteps": 218912,
    "episode_reward": -9.710828,
    "episode_length": 373
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 220163,
    "train/ent_coef": 0.034109339118003845,
    "train/actor_loss": 36.010284423828125,
    "train/critic_loss": 132.51809692382812,
    "train/ent_coef_loss": 0.9035341739654541,
    "episode": 1440,
    "total_timesteps": 220264,
    "episode_reward": -100.374295,
    "episode_length": 373
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 221443,
    "train/ent_coef": 0.032961178570985794,
    "train/actor_loss": 37.21581268310547,
    "train/critic_loss": 4.2234954833984375,
    "train/ent_coef_loss": -0.3609793186187744,
    "episode": 1450,
    "total_timesteps": 221544,
    "episode_reward": -53.028495,
    "episode_length": 364
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 222941,
    "train/ent_coef": 0.032298069447278976,
    "train/actor_loss": 36.39680480957031,
    "train/critic_loss": 16.437606811523438,
    "train/ent_coef_loss": -0.3242039084434509,
    "episode": 1460,
    "total_timesteps": 223042,
    "episode_reward": -51.261679,
    "episode_length": 370
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 225007,
    "train/ent_coef": 0.03508346900343895,
    "train/actor_loss": 37.34800720214844,
    "train/critic_loss": 1.0203495025634766,
    "train/ent_coef_loss": -1.6807948350906372,
    "episode": 1470,
    "total_timesteps": 225108,
    "episode_reward": -100.203696,
    "episode_length": 12
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 226353,
    "train/ent_coef": 0.03631815314292908,
    "train/actor_loss": 37.80670928955078,
    "train/critic_loss": 2.3810410499572754,
    "train/ent_coef_loss": -0.6868681311607361,
    "episode": 1480,
    "total_timesteps": 226454,
    "episode_reward": -100.04221,
    "episode_length": 3
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 227756,
    "train/ent_coef": 0.03577625751495361,
    "train/actor_loss": 37.50920104980469,
    "train/critic_loss": 6.217375755310059,
    "train/ent_coef_loss": -0.8346679210662842,
    "episode": 1490,
    "total_timesteps": 227857,
    "episode_reward": -100.014152,
    "episode_length": 1
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 228893,
    "train/ent_coef": 0.03323919698596001,
    "train/actor_loss": 36.662925720214844,
    "train/critic_loss": 14.274589538574219,
    "train/ent_coef_loss": 1.5012733936309814,
    "episode": 1500,
    "total_timesteps": 228994,
    "episode_reward": -46.715442,
    "episode_length": 373
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 230184,
    "train/ent_coef": 0.038800567388534546,
    "train/actor_loss": 38.863494873046875,
    "train/critic_loss": 2.872305154800415,
    "train/ent_coef_loss": 2.255666971206665,
    "episode": 1510,
    "total_timesteps": 230285,
    "episode_reward": -100.012763,
    "episode_length": 1
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 230856,
    "train/ent_coef": 0.04036318510770798,
    "train/actor_loss": 36.65184783935547,
    "train/critic_loss": 4.493215560913086,
    "train/ent_coef_loss": -0.7320170402526855,
    "episode": 1520,
    "total_timesteps": 230957,
    "episode_reward": -100.135535,
    "episode_length": 9
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 232162,
    "train/ent_coef": 0.038110360503196716,
    "train/actor_loss": 36.019569396972656,
    "train/critic_loss": 10.90972900390625,
    "train/ent_coef_loss": 0.7635481953620911,
    "episode": 1530,
    "total_timesteps": 232263,
    "episode_reward": -123.08681,
    "episode_length": 371
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 233411,
    "train/ent_coef": 0.04182589426636696,
    "train/actor_loss": 37.89984893798828,
    "train/critic_loss": 5.637598037719727,
    "train/ent_coef_loss": 1.3356437683105469,
    "episode": 1540,
    "total_timesteps": 233512,
    "episode_reward": -100.190746,
    "episode_length": 9
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 234424,
    "train/ent_coef": 0.04015335440635681,
    "train/actor_loss": 38.24852752685547,
    "train/critic_loss": 2.3622522354125977,
    "train/ent_coef_loss": -0.33011549711227417,
    "episode": 1550,
    "total_timesteps": 234525,
    "episode_reward": -100.037103,
    "episode_length": 3
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 235280,
    "train/ent_coef": 0.038243625313043594,
    "train/actor_loss": 37.393131256103516,
    "train/critic_loss": 4.460640907287598,
    "train/ent_coef_loss": -0.3294561505317688,
    "episode": 1560,
    "total_timesteps": 235381,
    "episode_reward": -100.012825,
    "episode_length": 1
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 236989,
    "train/ent_coef": 0.03624151647090912,
    "train/actor_loss": 37.53415298461914,
    "train/critic_loss": 1.393533706665039,
    "train/ent_coef_loss": -0.7597377896308899,
    "episode": 1570,
    "total_timesteps": 237090,
    "episode_reward": -93.086704,
    "episode_length": 373
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 238817,
    "train/ent_coef": 0.03695414215326309,
    "train/actor_loss": 37.128700256347656,
    "train/critic_loss": 9.143120765686035,
    "train/ent_coef_loss": 1.0462749004364014,
    "episode": 1580,
    "total_timesteps": 238918,
    "episode_reward": -100.013134,
    "episode_length": 1
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 241320,
    "train/ent_coef": 0.040512531995773315,
    "train/actor_loss": 38.22671127319336,
    "train/critic_loss": 79.60674285888672,
    "train/ent_coef_loss": -0.14548012614250183,
    "episode": 1590,
    "total_timesteps": 241421,
    "episode_reward": -161.01485,
    "episode_length": 362
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 242089,
    "train/ent_coef": 0.04070955142378807,
    "train/actor_loss": 38.328155517578125,
    "train/critic_loss": 33.533653259277344,
    "train/ent_coef_loss": 0.13985484838485718,
    "episode": 1600,
    "total_timesteps": 242190,
    "episode_reward": -81.572836,
    "episode_length": 371
  },
  {
    "train/learning_rate": 0.0003,
    "train/n_updates": 243596,
    "train/ent_coef": 0.03992379456758499,
    "train/actor_loss": 39.429237365722656,
    "train/critic_loss": 7.708604335784912,
    "train/ent_coef_loss": 1.1698967218399048,
    "episode": 1610,
    "total_timesteps": 243697,
    "episode_reward": -131.478084,
    "episode_length": 368
  }
]